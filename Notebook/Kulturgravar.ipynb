{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42398cc4-29cf-4f11-9e44-b8fd5db949e7",
   "metadata": {},
   "source": [
    "### Extrahera ut personer p책 Norra Begravningsplatsen fr책n kulturgravar\n",
    "\n",
    "* https://github.com/salgo60/Gravstensinventeringen-Wikidata/issues/39\n",
    "* https://www.kulturgravar.se\n",
    "\n",
    "TODO:\n",
    "* fixa b채ttre match med Wikidata\n",
    "* scanna alla gravar hos kulturgravar\n",
    "* ge alla kulturgravar ett unikt id kan noig vara dess sida\n",
    "* st채mma av vilka Wikidataobjekt vi redan har koppling till Kulturgravar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38e70e1-37b4-43e9-9af2-44a1b91d7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikibaseintegrator in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.12.8)\n",
      "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikibaseintegrator) (2.2.1)\n",
      "Requirement already satisfied: mwoauth<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikibaseintegrator) (0.4.0)\n",
      "Requirement already satisfied: oauthlib<4.0.0,>=3.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikibaseintegrator) (3.2.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikibaseintegrator) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikibaseintegrator) (2.0.0)\n",
      "Requirement already satisfied: ujson<6.0.0,>=5.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wikibaseintegrator) (5.10.0)\n",
      "Requirement already satisfied: PyJWT>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from mwoauth<0.5.0,>=0.4.0->wikibaseintegrator) (2.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->wikibaseintegrator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->wikibaseintegrator) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->wikibaseintegrator) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->wikibaseintegrator) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wikibaseintegrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfb52fe-b4ca-4824-b2c7-7075ef73d96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 subpages County:\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_vastragotalandslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_sodermanlandslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/index_om.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_uppsalalan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_hallandslan.html\n",
      "subpages:  ['https://www.kulturgravar.se/karta_hallandslan.html'] https://www.kulturgravar.se/karta_hallandslan.html\n",
      "subpages:  ['https://www.kulturgravar.se/karta_hallandslan.html'] ['https://www.kulturgravar.se/karta_hallandslan.html'] ['https://www.kulturgravar.se/karta_hallandslan.html']\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_ostergotlandslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_varmlandslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_blekingelan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/index_kontakt.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_orebrolan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_kronobergslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_dalarnaslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_jonkopingslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/krigsgravar.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_stockholmslan.html\n",
      "subpageCounty:  https://www.kulturgravar.se/karta_skaneslan.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def find_subpages(base_url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {base_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract all anchor tags\n",
    "    subpages = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        # Create full URL for relative links\n",
    "        full_url = urljoin(base_url, link[\"href\"])\n",
    "        if full_url.startswith(base_url):  # Ensure it's a subpage\n",
    "            subpages.append(full_url)\n",
    "\n",
    "    # Remove duplicates by converting to a set\n",
    "    subpages = list(set(subpages))\n",
    "    return subpages\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.kulturgravar.se\"\n",
    "\n",
    "# Find and print subpages\n",
    "\n",
    "subpagesCounty = find_subpages(base_url)\n",
    "print(f\"Found {len(subpagesCounty)} subpages County:\")\n",
    "for subpageCounty in subpagesCounty:\n",
    "    print(\"subpageCounty: \",subpageCounty)\n",
    "    subpages_county=find_subpages(subpageCounty)\n",
    "    for sub in subpages_county:\n",
    "        subpage2=find_subpages(sub)\n",
    "        print(\"subpages: \",subpage2,subpage)\n",
    "        for sub2 in subpage2:\n",
    "            subpage3=find_subpages(sub2)\n",
    "            print(\"subpages: \",subpage3,subpage2, subpage2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758c872-8cba-492e-9cc1-7d1a0ee4cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag \n",
    "# URL of the webpage to scrape\n",
    "urllist = {\n",
    "           \"https://www.kulturgravar.se/6_14_500_SV.html\",\n",
    "           \"https://www.kulturgravar.se/6_14_500_SO.html\"} #NorraBegravningsplatsen\n",
    "\n",
    "#urllist = {           \"https://www.kulturgravar.se/6_14_263_karta.html\",\n",
    "#           \"https://www.kulturgravar.se/6_14_500_SV.html\",\n",
    "#           \"https://www.kulturgravar.se/6_14_500_SO.html\"}\n",
    "\n",
    "url=\"https://www.kulturgravar.se\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d94326-5a05-49d6-851e-f10773854201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_points = []\n",
    "\n",
    "# Send a GET reaquest to fetch the page content\n",
    "for url in urllist:     \n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Page fetched successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "        exit()\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Locate the specific elements containing the list of people\n",
    "    # Based on the structure of the page source, adjust the selector\n",
    "    people = [] \n",
    "    \n",
    "    # Find all the required elements\n",
    "    points = soup.find_all('div', class_='point')\n",
    "    if points:  # Ensure current_results is not None\n",
    "        all_points.extend(points)\n",
    "\n",
    "\n",
    "# Extract details\n",
    "results = []\n",
    "#print(all_points)\n",
    "for points in  all_points:\n",
    "    #print(len(points), points)\n",
    "\n",
    "    for point in points:\n",
    "        # Ensure the current element is a bs4.element.Tag\n",
    "        if isinstance(point, Tag):\n",
    "            try:\n",
    "                # Extract the name and dates (from abbr title)\n",
    "                label_title = point.find('abbr')['title']\n",
    "            \n",
    "                # Append the result as a tuple\n",
    "                results.append(label_title)\n",
    "            except:\n",
    "                print(\"\\tError title\")\n",
    "                print(point)\n",
    "\n",
    "# Print the extracted data\n",
    "for person in results:\n",
    "    print(f\"Name and Dates: {person}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2782e2f-00f6-4c72-bffe-74705ab12105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eeb082-bf9a-4116-bcf4-20fb1a8ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikibaseintegrator.wbi_helpers import execute_sparql_query\n",
    "from wikibaseintegrator import WikibaseIntegrator \n",
    "from wikibaseintegrator.wbi_config import config as wbi_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7dad90-4cc1-489e-b61d-bae88e515497",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbi_config['USER_AGENT'] = 'WikibaseIntegrator in PAWS by salgo60'\n",
    "wbi = WikibaseIntegrator()\n",
    "\n",
    "def get_qnumber(name):\n",
    "        \n",
    "    # Query to search for the person by label\n",
    "    query = f\"\"\"\n",
    "    SELECT ?person ?personLabel WHERE {{\n",
    "        ?person ?label \"{name}\"@sv. # Swedish label\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"sv,en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    try:\n",
    "        results = execute_sparql_query(query)\n",
    "        bindings = results[\"results\"][\"bindings\"]\n",
    "        print(f\"Found {len(bindings)} results\")\n",
    "\n",
    "        if bindings:\n",
    "            return bindings[0]['person']['value'].split('/')[-1]  # Extract Qnumber\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Your list of results\n",
    "\n",
    "\n",
    "for person in results:\n",
    "    name_and_dates = person\n",
    "    name = name_and_dates.split(' (')[0]  # Extract name only\n",
    "    qnumber = get_qnumber(name)\n",
    "    \n",
    "    print(f\"File Link: {person[0]}\")\n",
    "    print(f\"Grave Title: {person[1]}\")\n",
    "    print(f\"Name and Dates: {name_and_dates}\")\n",
    "    print(f\"Wikidata Qnumber: {qnumber if qnumber else 'Not found'}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f12f7-c394-4192-adbf-9a702ad6608d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82006727-fedd-47f5-9fa2-7553e8a746b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
